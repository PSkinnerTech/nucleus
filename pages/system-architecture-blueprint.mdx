# System Architecture Blueprint

## üß± 1. High-Level Architecture Overview

```
+-------------------+          +------------------+          +-----------------------+
|   Voice Interface |  <--->   |  Rin Core AI     |  <--->   |     API Integrations  |
|   (Omi device)    |          |  (Brain/Agents)  |          |  (Calendar, Email, etc.)|
+-------------------+          +------------------+          +-----------------------+
          |                            |                                |
          v                            v                                v
+-------------------+       +---------------------+         +----------------------+
|  STT / TTS Engine | <---> |  MCP Server + Memory| <-->     |  Multimodal Models  |
|  (Omi, Whisper)   |       | (Session, Context)  |         | (LangChain / LLMs)  |
+-------------------+       +---------------------+         +----------------------+
```

---

## üß† 2. Core Components (with Roles)

### üó£Ô∏è Voice Layer
- Omi Wearable: Wake-word detection, speaker, microphone
- STT Engine (e.g., Whisper, Deepgram): Converts your voice into text
- TTS Engine (e.g., ElevenLabs, Google): Converts Rin's replies into voice
- Wake Word / Toggle System: Lightweight always-on listener on-device

---

### üß† Rin Core AI (Orchestration Engine)
- Prompt Router + Task Dispatcher: Determines intent (e.g. "Schedule lunch", "Order pizza")
- Personality Module: Prompt modifier + dynamic system messages
- Agent Orchestrator:
  - Background tasks via Celery
  - Scheduled agent chains
  - Parallel tasks with shared memory
- RLHF Feedback Training Module: Captures user corrections to improve future interactions

---

### üß© MCP Server + Context Engine
- Model Context Protocol (MCP): Handles per-thread context, user memory, task-specific context injection
- Long-term memory DB: Vector or hybrid store (e.g., Chroma, Weaviate, Postgres hybrid)
- Session memory: Stored per interaction (ephemeral + optional memory saving)

---

### üß† LLM Access & Tools
- LangChain or LiteLLM: Main access point for routing requests to optimal models
  - Fast, short responses ‚Üí Claude/Sonar
  - Long-context summaries ‚Üí GPT-4/Haiku
  - Coding ‚Üí Claude 2.1 or GPT-4
- Model switcher: Fallback to local LLMs (e.g., Ollama) when offline or private

---

### üîå API Integrations (via Plugin Layer)
- Calendar: Google, Outlook
- Email: Gmail (OAuth), Outlook
- Social: X, LinkedIn (via Graph APIs or 3rd party)
- Messaging: Telegram, Discord, SMS (Twilio)
- Food/Services: UberEats, Rappi, Amazon
- Custom: Python scripts, IFTTT, Zapier webhooks

---

### üîí Security Layer
- Encrypted token vault (e.g., HashiCorp Vault, Python-keyring, or local encrypted store)
- Auth abstraction for multiple providers (OAuth2, API key headers)
- Optional MFA fallback, passcode gate, or voiceprint verification

---

## üîÑ 3. Data Flow Example

Use Case: "Rin, schedule lunch with Nani at 1pm tomorrow."
1. Omi listens ‚Üí Wake word triggers active listening
2. Voice input ‚Üí STT (Whisper) ‚Üí "schedule lunch with Nani at 1pm tomorrow"
3. Rin Core AI:
   - Detects intent (schedule task)
   - Uses memory: knows "Nani = Stephanie, wife"
4. Calls Calendar API: Authenticated call to Google Calendar
5. Creates event, checks for conflicts, confirms aloud
6. Text response ‚Üí TTS (Google Cloud TTS) ‚Üí "Done. Lunch with Nani scheduled."
7. Optional: stores memory of lunch preference for future suggestions

---

## üß∞ 4. Suggested Tech Stack

| Component | Recommended Tools |
| --------- | ----------------- |
| Voice SDK | Omi SDK, Whisper (STT), ElevenLabs (TTS) |
| LLMs (API) | LangChain/LiteLLM, OpenAI Python SDK, Anthropic SDK, Ollama (offline fallback) |
| MCP Server | Self-hosted Python server, deployable with Docker |
| Memory DB | Chroma, Weaviate, or PostgreSQL Hybrid |
| Backend API | FastAPI or Flask |
| Frontend (Optional) | React + Tailwind (web) or PyQt/Tkinter for local GUI |
| Hosting Model | Hybrid (Self-hosted MCP + memory, managed LLMs) or fully local for high privacy |
| Auth Layer | FastAPI Auth, Python-jose, Vault |
| Agent Runtime | LangChain, LlamaIndex, or custom Python orchestration |
| Scheduler / Queue | Celery or RQ with Redis backend |
| Agent Permissions | Configurable per task ‚Äî open-ended agents may ask for permission optionally |